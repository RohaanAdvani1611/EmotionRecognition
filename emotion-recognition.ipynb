{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\n\nfrom keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\nfrom keras.models import Model,Sequential\nfrom tensorflow.keras.optimizers import Adam,SGD,RMSprop\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2022-11-14T15:54:59.502791Z","iopub.execute_input":"2022-11-14T15:54:59.503576Z","iopub.status.idle":"2022-11-14T15:55:09.019450Z","shell.execute_reply.started":"2022-11-14T15:54:59.503461Z","shell.execute_reply":"2022-11-14T15:55:09.018065Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"picture_size = 48\nfolder_path = \"../input/face-expression-recognition-dataset/images/\"","metadata":{"execution":{"iopub.status.busy":"2022-11-14T15:55:09.021443Z","iopub.execute_input":"2022-11-14T15:55:09.022591Z","iopub.status.idle":"2022-11-14T15:55:09.029558Z","shell.execute_reply.started":"2022-11-14T15:55:09.022519Z","shell.execute_reply":"2022-11-14T15:55:09.028030Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Batch Size - How many Training images model will take in 1 iteration\nbatch_size  = 128\n\n# Define Image Data Generator variables\ndatagen_train  = ImageDataGenerator()\ndatagen_val = ImageDataGenerator()\n\n# Store images in folder path in the training and validation sets - 7 categories of expressions\ntrain_set = datagen_train.flow_from_directory(folder_path+\"train\",\n                                              target_size = (picture_size,picture_size),\n                                              color_mode = \"grayscale\",\n                                              batch_size=batch_size,\n                                              class_mode='categorical',\n                                              shuffle=True)\ntest_set = datagen_val.flow_from_directory(folder_path+\"validation\",\n                                              target_size = (picture_size,picture_size),\n                                              color_mode = \"grayscale\",\n                                              batch_size=batch_size,\n                                              class_mode='categorical',\n                                              shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T15:55:09.031737Z","iopub.execute_input":"2022-11-14T15:55:09.032287Z","iopub.status.idle":"2022-11-14T15:55:33.040142Z","shell.execute_reply.started":"2022-11-14T15:55:09.032234Z","shell.execute_reply":"2022-11-14T15:55:33.038853Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Found 28821 images belonging to 7 classes.\nFound 7066 images belonging to 7 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"# There are 7 classes/ possible outcomes\nno_of_classes = 7\n\nmodel = Sequential()\n\n# STEP 1: Conv2D(no_filters,kernel_size,padding,input_shape(picture_size,picture_size,grayscale=1))\n# filters: Dimensionality of output space (number of output filters in the convolution).\n# kernel_size: Tuple of 2 ints, height and width of the 2D convolution window.\n\n# STEP 2: Batch normalization normalizes its inputs, that maintains the mean output close to 0 \n# and the output standard deviation close to 1.\n\n# STEP 3: Activation layer is present at end of neuron, it decides information fired to the next layer\n# relu - Recitified Linear Unit - Chosen as it does not trigger all neurons at same time\n\n# STEP 4: Max pooling operation for 2D spatial data.\n# Downsamples the input along its spatial dimensions (height and width) by taking the maximum \n# value over an input window (pool_size) for each channel of the input.\n\n# STEP 5: Dropout sets input to 0 with frequency of rate at each step during training, preventing overfitting. \n# Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n\n#1st CNN layer\nmodel.add(Conv2D(64,(3,3),padding = 'same',input_shape = (picture_size,picture_size,1)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(Dropout(0.25))\n\n#2nd CNN layer\nmodel.add(Conv2D(128,(5,5),padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(Dropout (0.25))\n\n#3rd CNN layer\nmodel.add(Conv2D(512,(3,3),padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(Dropout (0.25))\n\n#4th CNN layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# STEP 6: Flatten Layer - Collapses input to 1D array which can be fed in model easily\nmodel.add(Flatten())\n\n# STEP 7: Use Dense Layer Connect all the layers\n# units: Positive integer, dimensionality of the output space.\n# Dense implements the operation: output = activation(dot(input, kernel) + bias) \n# where activation is the element-wise activation function passed as the activation argument, \n# kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer \n# (only applicable if use_bias is True).\n#Fully connected 1st layer\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n# Fully connected layer 2nd layer\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n# Softmax is used for categorical classification\nmodel.add(Dense(no_of_classes, activation='softmax'))\n\n# STEP 8: Optimize model use learning rate 0.0001\n# Set Loss Function as cross entropy - used with softmax activation\n# Optimizer that implements the Adam algorithm.\n# Adam optimization is a stochastic gradient descent method that is based on \n# adaptive estimation of first-order and second-order moments.\nopt = Adam(lr = 0.0001)\nmodel.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-11-14T15:55:33.043112Z","iopub.execute_input":"2022-11-14T15:55:33.043943Z","iopub.status.idle":"2022-11-14T15:55:33.499084Z","shell.execute_reply.started":"2022-11-14T15:55:33.043890Z","shell.execute_reply":"2022-11-14T15:55:33.497674Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2022-11-14 15:55:33.102334: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 48, 48, 64)        640       \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 48, 48, 64)        256       \n_________________________________________________________________\nactivation (Activation)      (None, 48, 48, 64)        0         \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 24, 24, 64)        0         \n_________________________________________________________________\ndropout (Dropout)            (None, 24, 24, 64)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 24, 24, 128)       204928    \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 24, 24, 128)       512       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 24, 24, 128)       0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 12, 12, 128)       0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 12, 12, 128)       0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 12, 12, 512)       590336    \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 12, 12, 512)       2048      \n_________________________________________________________________\nactivation_2 (Activation)    (None, 12, 12, 512)       0         \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 6, 6, 512)         0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 6, 6, 512)         0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 6, 6, 512)         2359808   \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 6, 6, 512)         2048      \n_________________________________________________________________\nactivation_3 (Activation)    (None, 6, 6, 512)         0         \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 3, 3, 512)         0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 3, 3, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 4608)              0         \n_________________________________________________________________\ndense (Dense)                (None, 256)               1179904   \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 256)               1024      \n_________________________________________________________________\nactivation_4 (Activation)    (None, 256)               0         \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 512)               131584    \n_________________________________________________________________\nbatch_normalization_5 (Batch (None, 512)               2048      \n_________________________________________________________________\nactivation_5 (Activation)    (None, 512)               0         \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 7)                 3591      \n=================================================================\nTotal params: 4,478,727\nTrainable params: 4,474,759\nNon-trainable params: 3,968\n_________________________________________________________________\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# STEP 9: ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or \n# weights (in a checkpoint file) at some interval, so the model or weights can be loaded later \n# to continue the training from the state saved.\n# Keras verbose defines the mode of verbosity, which will be auto 0, 1, or 2. \n# In this mode, 0 is defined as silent, 1 as a progress bar, and 2 as a single line per epoch.\n# if save_best_only=True, it only saves when the model is considered the \"best\" \n# and the latest best model according to the quantity monitored will not be overwritten.\n# mode: one of {'auto', 'min', 'max'}. If save_best_only=True, the decision to overwrite the current \n# save file is made based on either the maximization or the minimization of the monitored quantity.\ncheckpoint = ModelCheckpoint(\"./model.h5\",monitor='val_acc',verbose=1,save_best_only=True,mode='max')\n\n# STEP 10: Early Stopping - Stop training when a monitored metric has stopped improving.\n# min_delta: Min change in monitored quantity to qualify as improvement, \n# less than min_delta is no improvement.\n# patience: Number of epochs with no improvement after which training will be stopped.\n# restore_best_weights: Restore model weights from epoch with best value of monitored quantity. \nearly_stopping = EarlyStopping(monitor='val_loss',min_delta=0,patience=3,verbose=1,restore_best_weights=True)\n\n# STEP 11: Reduce learning rate when a metric has stopped improving.\n# Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates.\nreduce_learningrate = ReduceLROnPlateau(monitor='val_loss',factor=0.2,patience=3,verbose=1,min_delta=0.0001)\n\ncallbacks_list = [early_stopping,checkpoint,reduce_learningrate]\nepochs = 48\n# Compile Model\nmodel.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.001),metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-11-14T15:55:33.501130Z","iopub.execute_input":"2022-11-14T15:55:33.501631Z","iopub.status.idle":"2022-11-14T15:55:33.518998Z","shell.execute_reply.started":"2022-11-14T15:55:33.501584Z","shell.execute_reply":"2022-11-14T15:55:33.517508Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"history = model.fit_generator(generator=train_set,\n                                steps_per_epoch=train_set.n//train_set.batch_size,\n                                epochs=epochs,\n                                validation_data = test_set,\n                                validation_steps = test_set.n//test_set.batch_size,\n                                callbacks=callbacks_list)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T15:55:33.521205Z","iopub.execute_input":"2022-11-14T15:55:33.521689Z","iopub.status.idle":"2022-11-14T17:19:24.964527Z","shell.execute_reply.started":"2022-11-14T15:55:33.521651Z","shell.execute_reply":"2022-11-14T17:19:24.962872Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  warnings.warn('`Model.fit_generator` is deprecated and '\n2022-11-14 15:55:34.320760: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/48\n225/225 [==============================] - 549s 2s/step - loss: 1.8131 - accuracy: 0.3049 - val_loss: 1.7419 - val_accuracy: 0.3399\nEpoch 2/48\n225/225 [==============================] - 537s 2s/step - loss: 1.4589 - accuracy: 0.4413 - val_loss: 1.4729 - val_accuracy: 0.4222\nEpoch 3/48\n225/225 [==============================] - 536s 2s/step - loss: 1.2898 - accuracy: 0.5049 - val_loss: 1.3376 - val_accuracy: 0.4885\nEpoch 4/48\n225/225 [==============================] - 531s 2s/step - loss: 1.1952 - accuracy: 0.5458 - val_loss: 1.1756 - val_accuracy: 0.5504\nEpoch 5/48\n225/225 [==============================] - 527s 2s/step - loss: 1.1351 - accuracy: 0.5697 - val_loss: 1.2225 - val_accuracy: 0.5477\nEpoch 6/48\n225/225 [==============================] - 527s 2s/step - loss: 1.0836 - accuracy: 0.5866 - val_loss: 1.1042 - val_accuracy: 0.5828\nEpoch 7/48\n225/225 [==============================] - 526s 2s/step - loss: 1.0410 - accuracy: 0.6054 - val_loss: 1.4623 - val_accuracy: 0.4432\nEpoch 8/48\n225/225 [==============================] - 533s 2s/step - loss: 0.9943 - accuracy: 0.6224 - val_loss: 1.2363 - val_accuracy: 0.5355\nEpoch 9/48\n225/225 [==============================] - 533s 2s/step - loss: 0.9526 - accuracy: 0.6409 - val_loss: 1.1592 - val_accuracy: 0.5722\nRestoring model weights from the end of the best epoch.\n\nEpoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\nEpoch 00009: early stopping\n","output_type":"stream"}]}]}